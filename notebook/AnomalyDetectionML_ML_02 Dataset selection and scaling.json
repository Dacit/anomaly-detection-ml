{"paragraphs":[{"text":"%md\n\n# 1. Split into train and validate set\n","user":"anonymous","dateUpdated":"2017-12-05T14:51:15+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>1. Split into train and validate set</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1504170286535_-1402507970","id":"20170831-110446_2118383950","dateCreated":"2017-08-31T11:04:46+0200","dateStarted":"2017-12-05T14:51:09+0100","dateFinished":"2017-12-05T14:51:09+0100","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:26"},{"text":"%spark\nval splitDate = Timestamp.valueOf(\"2015-06-01 00:00:00\")\n\nval fullRawTrainingMultilabeled = labeledDataset.filter(_.ts.before(splitDate)).persist(StorageLevel.OFF_HEAP)\nval validateRawMultilabeled = labeledDataset.filter(_.ts.after(splitDate))","user":"anonymous","dateUpdated":"2017-12-05T12:49:33+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"splitDate: java.sql.Timestamp = 2015-06-01 00:00:00.0\nfullRawTrainingMultilabeled: org.apache.spark.sql.Dataset[Labeled] = [host: string, procs: string ... 3 more fields]\nvalidateRawMultilabeled: org.apache.spark.sql.Dataset[Labeled] = [host: string, procs: string ... 3 more fields]\n"}]},"apps":[],"jobName":"paragraph_1504170302840_826373057","id":"20170831-110502_2079236041","dateCreated":"2017-08-31T11:05:02+0200","dateStarted":"2017-12-05T12:49:33+0100","dateFinished":"2017-12-05T13:00:00+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:27"},{"text":"%md\n\n## 1.1 Some statistics:\n","user":"anonymous","dateUpdated":"2017-12-05T14:53:10+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>1.1 Some statistics:</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1506275739932_336263662","id":"20170924-195539_670009312","dateCreated":"2017-09-24T19:55:39+0200","dateStarted":"2017-12-05T14:51:11+0100","dateFinished":"2017-12-05T14:51:11+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28"},{"text":"%spark\n{\nval trainingNormalCount = fullRawTrainingMultilabeled.filter($\"manualLabel\" === 0).count.toDouble\nval trainingAnomalyCount = fullRawTrainingMultilabeled.filter($\"manualLabel\" > 0).count.toDouble\nval validateCount = validateRawMultilabeled.count.toDouble\nprintln(\"train/test ratio: \" + ((trainingNormalCount + trainingAnomalyCount) / (trainingNormalCount + trainingAnomalyCount + validateCount)) +  \"/\" + (validateCount / (trainingNormalCount + trainingAnomalyCount + validateCount)))\nprintln(\"default training normal/anomaly ratio: \" + (trainingNormalCount / (trainingNormalCount + trainingAnomalyCount)) + \"/\" + (trainingAnomalyCount / (trainingNormalCount + trainingAnomalyCount)))\n}","user":"anonymous","dateUpdated":"2017-12-05T14:52:35+0100","config":{"colWidth":12,"enabled":false,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"train/test ratio: 0.593577942231491/0.40642205776850904\ndefault training normal/anomaly ratio: 0.9872201172051399/0.012779882794860078\n"}]},"apps":[],"jobName":"paragraph_1506275734311_-230871541","id":"20170924-195534_1638540325","dateCreated":"2017-09-24T19:55:34+0200","dateStarted":"2017-09-24T20:12:05+0200","dateFinished":"2017-09-24T20:12:07+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:29"},{"text":"%md\n\n# 2. Scale (scaler must only learn from training)","user":"anonymous","dateUpdated":"2017-12-05T14:53:33+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1504170835545_1253222696","id":"20170831-111355_1016579437","dateCreated":"2017-08-31T11:13:55+0200","dateStarted":"2017-12-05T14:53:32+0100","dateFinished":"2017-12-05T14:53:32+0100","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:30"},{"text":"%spark\nimport org.apache.spark.ml.feature._\n\nval scaler = new StandardScaler()\n    .setInputCol(\"features\")\n    .setOutputCol(\"scaledFeatures\")\n    .setWithStd(true)\n    .setWithMean(true)\n    .fit(fullRawTrainingMultilabeled)\n    \nval fullTraining = scaler.transform(fullRawTrainingMultilabeled).withColumn(\"manualLabel\", when($\"manualLabel\" > 0, 1.0).otherwise(0.0)).persist(StorageLevel.OFF_HEAP)\n// Finished, so disk spilling is ok here\nval validate = scaler.transform(validateRawMultilabeled).withColumn(\"manualLabel\", when($\"manualLabel\" > 0, 1.0).otherwise(0.0)).cache","user":"anonymous","dateUpdated":"2017-12-05T12:49:33+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.ml.feature._\nscaler: org.apache.spark.ml.feature.StandardScalerModel = stdScal_55a8c0505f13\nfullTraining: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [host: string, procs: string ... 4 more fields]\nvalidate: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [host: string, procs: string ... 4 more fields]\n"}]},"apps":[],"jobName":"paragraph_1504170849553_-1220312659","id":"20170831-111409_2014551432","dateCreated":"2017-08-31T11:14:09+0200","dateStarted":"2017-12-05T12:59:59+0100","dateFinished":"2017-12-05T13:00:11+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:31"},{"text":"%md\n\n# 3. Select while retaining time series property","user":"anonymous","dateUpdated":"2017-12-05T14:53:43+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>3. Select while retaining time series property</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1504169947505_601606367","id":"20170831-105907_529793781","dateCreated":"2017-08-31T10:59:07+0200","dateStarted":"2017-12-05T12:49:33+0100","dateFinished":"2017-12-05T12:49:33+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:32"},{"text":"%spark\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.catalyst.encoders.RowEncoder\n\ndef dayGroupFn(r: Row): String = {\n    r.getAs[String](\"host\") + r.getAs[String](\"procs\") + r.getAs[Timestamp](\"ts\").toString.substring(0, 11)\n}\n\n// Find days on which anomalies occurred\nval anomalyDays = fullTraining.filter($\"manualLabel\" > 0)\n    \n// Find days on which no anomalies occurred, \nval normalDays = fullTraining\n    .filter($\"manualLabel\" === 0)\n    .sample(false, 0.001)\n    .limit(10)\n    .flatMap(r => {\n        val before = Row.fromSeq(r.toSeq.patch(r.fieldIndex(\"ts\"), Seq(new Timestamp(r.getAs[Timestamp](\"ts\").getTime - (24*60*60*1000))), 1))\n        val after = Row.fromSeq(r.toSeq.patch(r.fieldIndex(\"ts\"), Seq(new Timestamp(r.getAs[Timestamp](\"ts\").getTime + (24*60*60*1000))), 1))\n        Iterator(before, r, after)\n    }) (RowEncoder.apply(fullTraining.schema))\n\nval selectedDays = anomalyDays.union(normalDays).groupByKey(dayGroupFn(_))\n\nval training = fullTraining.groupByKey(dayGroupFn(_))\n    .cogroup(selectedDays) ((k: String, xs: Iterator[Row], zs: Iterator[Row]) => {\n        if (zs.size > 0) xs else Iterator()\n    }) (RowEncoder.apply(fullTraining.schema))\n    .cache\n    \n\ntraining.count\nvalidate.count\n\n{\nval anomalyCount = training.filter($\"manualLabel\" > 0).count.toDouble\nval normalCount = training.filter($\"manualLabel\" === 0).count.toDouble\n\nprintln(\"selected training ratio of initial: \" + (anomalyCount.toDouble + normalCount.toDouble) / fullTraining.count)\nprintln(\"selected training normal/anomaly ratio: \" + normalCount / (normalCount + anomalyCount) + \"/\" + anomalyCount / (normalCount + anomalyCount))\n}","user":"anonymous","dateUpdated":"2017-12-05T12:49:33+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.Row\nimport org.apache.spark.sql.catalyst.encoders.RowEncoder\ndayGroupFn: (r: org.apache.spark.sql.Row)String\nanomalyDays: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [host: string, procs: string ... 4 more fields]\nnormalDays: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [host: string, procs: string ... 4 more fields]\nselectedDays: org.apache.spark.sql.KeyValueGroupedDataset[String,org.apache.spark.sql.Row] = org.apache.spark.sql.KeyValueGroupedDataset@4eaf87a5\ntraining: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [host: string, procs: string ... 4 more fields]\nres76: Long = 167358\nres77: Long = 3058082\nselected training ratio of initial: 0.037490767765708574\nselected training normal/anomaly ratio: 0.6573453315646698/0.34265466843533027\n"}]},"apps":[],"jobName":"paragraph_1504169905486_-1998203247","id":"20170831-105825_1681208583","dateCreated":"2017-08-31T10:58:25+0200","dateStarted":"2017-12-05T13:00:00+0100","dateFinished":"2017-12-05T13:01:01+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33"},{"text":"%md\n\n## Write to file","user":"anonymous","dateUpdated":"2017-12-05T14:54:03+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Write to file</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1504186118702_-1170012806","id":"20170831-152838_1458014837","dateCreated":"2017-08-31T15:28:38+0200","dateStarted":"2017-12-05T12:49:33+0100","dateFinished":"2017-12-05T12:49:33+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:34"},{"text":"%spark\ntraining.show","user":"anonymous","dateUpdated":"2017-12-05T12:49:33+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------+-----+--------------------+--------------------+-----------+--------------------+\n|    host|procs|                  ts|            features|manualLabel|      scaledFeatures|\n+--------+-----+--------------------+--------------------+-----------+--------------------+\n|lphost09| wls2|2014-12-22 01:45:...|[1.0,0.0,0.542968...|        0.0|[0.05663485433935...|\n|lphost09| wls2|2014-12-22 06:23:...|[1.0,0.0,0.580729...|        0.0|[0.05663485433935...|\n|lphost09| wls2|2014-12-22 08:39:...|[1.0,0.0,0.582031...|        0.0|[0.05663485433935...|\n|lphost09| wls2|2014-12-22 11:32:...|[1.0,0.0,0.582031...|        0.0|[0.05663485433935...|\n|lphost09| wls2|2014-12-22 12:10:...|[1.0,0.0,0.582031...|        0.0|[0.05663485433935...|\n|lphost09| wls2|2014-12-22 14:05:...|[1.0,0.0,0.582031...|        0.0|[0.05663485433935...|\n|lphost09| wls2|2014-12-22 14:26:...|[1.0,0.0,0.582031...|        0.0|[0.05663485433935...|\n|lphost09| wls2|2014-12-22 15:05:...|[1.0,0.0,0.582031...|        0.0|[0.05663485433935...|\n|lphost09| wls2|2014-12-22 15:39:...|[1.0,0.0,0.582031...|        0.0|[0.05663485433935...|\n|lphost09| wls2|2014-12-22 19:09:...|[1.0,0.0,0.582031...|        0.0|[0.05663485433935...|\n|lphost09| wls2|2014-12-22 01:52:...|[1.0,0.0,0.542968...|        0.0|[0.05663485433935...|\n|lphost09| wls2|2014-12-22 02:08:...|[1.0,0.0,0.542968...|        0.0|[0.05663485433935...|\n|lphost09| wls2|2014-12-22 03:07:...|[1.0,0.0,0.542968...|        0.0|[0.05663485433935...|\n|lphost09| wls2|2014-12-22 03:45:...|[1.0,0.0,0.542968...|        0.0|[0.05663485433935...|\n|lphost09| wls2|2014-12-22 12:51:...|[1.0,0.0,0.582031...|        0.0|[0.05663485433935...|\n|lphost09| wls2|2014-12-22 16:11:...|[1.0,0.0,0.582031...|        0.0|[0.05663485433935...|\n|lphost09| wls2|2014-12-22 23:14:...|[1.0,0.0,0.582031...|        0.0|[0.05663485433935...|\n|lphost09| wls2|2014-12-22 00:57:...|[1.0,0.0,0.542968...|        0.0|[0.05663485433935...|\n|lphost09| wls2|2014-12-22 07:05:...|[1.0,0.0,0.580729...|        0.0|[0.05663485433935...|\n|lphost09| wls2|2014-12-22 07:55:...|[1.0,0.0,0.580729...|        0.0|[0.05663485433935...|\n+--------+-----+--------------------+--------------------+-----------+--------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1504535026292_1072570401","id":"20170904-162346_1316881731","dateCreated":"2017-09-04T16:23:46+0200","dateStarted":"2017-12-05T13:00:11+0100","dateFinished":"2017-12-05T13:01:01+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:35"},{"text":"%spark\ntraining.rdd.saveAsObjectFile(\"/anomaly-detection-ml/training\")\nvalidate.rdd.saveAsObjectFile(\"/anomaly-detection-ml/validate\")","user":"anonymous","dateUpdated":"2017-12-05T12:49:33+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1504250073332_-1089727879","id":"20170901-091433_1958752456","dateCreated":"2017-09-01T09:14:33+0200","dateStarted":"2017-12-05T13:01:01+0100","dateFinished":"2017-12-05T13:02:28+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:36"},{"user":"anonymous","dateUpdated":"2017-12-05T12:49:33+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1505916642103_-1183414181","id":"20170920-161042_24475772","dateCreated":"2017-09-20T16:10:42+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37"}],"name":"AnomalyDetectionML/ML/02 Dataset selection and scaling","id":"2CTAWM4TF","angularObjects":{"2CZ6JJTDR:shared_process":[],"2CZU9K4SP:shared_process":[],"2CYR4Y5D7:shared_process":[],"2D33TFRH4:shared_process":[],"2D18HK2TW:shared_process":[],"2D15EVSM5:shared_process":[],"2D2FQENTY:shared_process":[],"2CY5C8M4U:shared_process":[],"2CZGJ6DT8:shared_process":[],"2D2H4Y35Y:shared_process":[],"2D21J2RXK:shared_process":[],"2D2QUK9JC:shared_process":[],"2CYJWH63V:shared_process":[],"2D2XZGK45:shared_process":[],"2D1F6X9XF:shared_process":[],"2CY8V4V41:shared_process":[],"2D1JVM9EC:shared_process":[],"2D2NZHKM4:shared_process":[],"2D2E56F9K:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}