{"paragraphs":[{"text":"%kryo-spark.dep\n\nz.reset\nz.addRepo(\"Spark Packages Repo\").url(\"https://dl.bintray.com/spark-packages/maven\")\nz.addRepo(\"qaware-internal-snapshots\").url(\"https://www.qaware.de/nexus/content/repositories/qaware-internal-snapshots/\").username(\"f.huch\").password(\"\")\nz.load(\"de.qaware.mlwb:featureextractor:1.0-SNAPSHOT\")\nz.load(\"de.qaware.mlwb:mlwb-impl:1.2-SNAPSHOT\")\nz.load(\"org.nd4j:nd4j-kryo_2.11:0.9.1\")\nz.load(\"org.datavec:datavec-api:0.9.1\")\nz.load(\"org.apache.mahout:mahout-spark_2.10:0.13.0\")\nz.load(\"org.apache.commons:commons-math3:3.6.1\")\nz.load(\"org.vegas-viz:vegas_2.11:0.3.11\")\nz.load(\"org.vegas-viz:vegas-spark_2.11:0.3.11\")\nz.load(\"sramirez:spark-infotheoretic-feature-selection:1.4.0\")\nz.load(\"org.nd4j:nd4j-native:0.9.1\")\nz.load(\"org.deeplearning4j:dl4j-spark_2.11:0.9.1_spark_2\")\n\nz.fetch","user":"anonymous","dateUpdated":"2017-12-18T11:34:37+0100","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res0: java.util.List[java.io.File] = [/opt/zeppelin/local-repo/de/qaware/mlwb/featureextractor/1.0-SNAPSHOT/featureextractor-1.0-SNAPSHOT.jar, /opt/zeppelin/local-repo/de/qaware/mlwb/mlwb-impl/1.2-SNAPSHOT/mlwb-impl-1.2-SNAPSHOT.jar, /opt/zeppelin/local-repo/de/qaware/mlwb/mlwb-api/1.2-SNAPSHOT/mlwb-api-1.2-SNAPSHOT.jar, /opt/zeppelin/local-repo/org/apache/commons/commons-lang3/3.4/commons-lang3-3.4.jar, /opt/zeppelin/local-repo/de/qaware/mlwb/mlwb-dt/1.2-SNAPSHOT/mlwb-dt-1.2-SNAPSHOT.jar, /opt/zeppelin/local-repo/org/apache/solr/solr-solrj/6.5.1/solr-solrj-6.5.1.jar, /opt/zeppelin/local-repo/org/apache/httpcomponents/httpclient/4.4.1/httpclient-4.4.1.jar, /opt/zeppelin/local-repo/org/apache/httpcomponents/httpcore/4.4.1/httpcore-4.4.1.jar, /opt/zeppelin/local-repo/org/apache/httpcompon..."}]},"apps":[],"jobName":"paragraph_1505309655705_-316244972","id":"20170706-112009_1863775061","dateCreated":"2017-09-13T15:34:15+0200","dateStarted":"2017-12-18T11:34:37+0100","dateFinished":"2017-12-18T11:34:48+0100","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:26"},{"text":"%kryo-spark\nprint(sc)","user":"anonymous","dateUpdated":"2017-12-18T11:34:37+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"org.apache.spark.SparkContext@74c424be"}]},"apps":[],"jobName":"paragraph_1512406001565_1910053456","id":"20171204-174641_1397126197","dateCreated":"2017-12-04T17:46:41+0100","dateStarted":"2017-12-18T11:34:38+0100","dateFinished":"2017-12-18T11:35:03+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:27"},{"text":"%kryo-spark\nimport org.apache.spark.sql.Dataset\nimport org.apache.spark.sql.Encoders\nimport java.sql.Timestamp\nimport com.google.common.collect.Iterators\nimport de.qaware.mlwb.api._\nimport collection.JavaConverters._\nimport util.control.Breaks._\nimport org.apache.spark.sql.SaveMode\nimport de.qaware.mlwb.impl.sparksolr.MetricsServiceImpl\nimport org.apache.spark.sql.SaveMode\nimport org.apache.commons.math3.analysis.interpolation.HermiteInterpolator\nimport scala.util.control.Breaks._\nimport java.util.function.Consumer","user":"anonymous","dateUpdated":"2017-12-18T11:34:38+0100","config":{"tableHide":true,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.Dataset\nimport org.apache.spark.sql.Encoders\nimport java.sql.Timestamp\nimport com.google.common.collect.Iterators\nimport de.qaware.mlwb.api._\nimport collection.JavaConverters._\nimport util.control.Breaks._\nimport org.apache.spark.sql.SaveMode\nimport de.qaware.mlwb.impl.sparksolr.MetricsServiceImpl\nimport org.apache.spark.sql.SaveMode\nimport org.apache.commons.math3.analysis.interpolation.HermiteInterpolator\nimport scala.util.control.Breaks._\nimport java.util.function.Consumer\n"}]},"apps":[],"jobName":"paragraph_1505309655706_-315090726","id":"20170718-155252_1934180693","dateCreated":"2017-09-13T15:34:15+0200","dateStarted":"2017-12-18T11:34:49+0100","dateFinished":"2017-12-18T11:35:06+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28"},{"text":"%md\n\n# Feature definition","user":"anonymous","dateUpdated":"2017-12-18T11:34:38+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Feature definition</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1512404904065_725478943","id":"20171204-172824_934118544","dateCreated":"2017-12-04T17:28:24+0100","dateStarted":"2017-12-18T11:34:38+0100","dateFinished":"2017-12-18T11:34:38+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:29"},{"text":"%kryo-spark\nabstract sealed class Combinator extends Serializable\ncase object Addition extends Combinator\ncase object Substraction extends Combinator\ncase object Multiplication extends Combinator\ncase object Division extends Combinator\n\nabstract sealed class Feature extends Serializable\nsealed case class MetricKey(name: String, propertyNames: Set[String]) extends Feature {\n    def toQueryString(): String = {\n        \"*\" + propertyNames.filter(!Set(\"series\", \"host\", \"process\").contains(_)).toList.sortBy(_.toLowerCase).reduceOption(_ + \"*\" + _).getOrElse(\"\") + \"*\" + name\n    }\n}\nsealed case class NamedFeature(name: String, f: Feature) extends Feature\nsealed case class Derivative(f: Feature) extends Feature\nsealed case class Inclination(f: Feature) extends Feature\nabstract sealed class Combination extends Feature\nsealed case class BiCombination(f1: Feature, f2: Feature, c: Combinator) extends Combination\nsealed case class LeftCombination(f: Feature, d: Double, c: Combinator) extends Combination\nsealed case class RightCombination(d: Double, f: Feature, c: Combinator) extends Combination\n\ndef extractMetrics(feature: Feature): Set[MetricKey] = feature match {\n    case MetricKey(n,p) => Set(MetricKey(n,p))\n    case NamedFeature(n, f) => extractMetrics(f)\n    case Derivative(f) => extractMetrics(f)\n    case Inclination(f) => extractMetrics(f)\n    case BiCombination(f1, f2, _) => extractMetrics(f1) ++ extractMetrics(f2)\n    case LeftCombination(f, _, _) => extractMetrics(f)\n    case RightCombination(_, f, _) => extractMetrics(f)\n}","user":"anonymous","dateUpdated":"2017-12-18T11:34:38+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"defined class Combinator\ndefined object Addition\ndefined object Substraction\ndefined object Multiplication\ndefined object Division\ndefined class Feature\ndefined class MetricKey\ndefined class NamedFeature\ndefined class Derivative\ndefined class Inclination\ndefined class Combination\ndefined class BiCombination\ndefined class LeftCombination\ndefined class RightCombination\n<console>:64: warning: match may not be exhaustive.\nIt would fail on the following inputs: BiCombination(_, _, _), Derivative(_), Inclination(_), LeftCombination(_, _, _), MetricKey(_, _), NamedFeature(_, _), RightCombination(_, _, _)\n       def extractMetrics(feature: Feature): Set[MetricKey] = feature match {\n                                                              ^\nextractMetrics: (feature: Feature)Set[MetricKey]\n"}]},"apps":[],"jobName":"paragraph_1512404913833_1262218983","id":"20171204-172833_694695977","dateCreated":"2017-12-04T17:28:33+0100","dateStarted":"2017-12-18T11:35:04+0100","dateFinished":"2017-12-18T11:35:09+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:30"},{"text":"%md\n\n# Interpolation","user":"anonymous","dateUpdated":"2017-12-18T11:34:38+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Interpolation</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1512404949209_-1560116702","id":"20171204-172909_1898593438","dateCreated":"2017-12-04T17:29:09+0100","dateStarted":"2017-12-18T11:34:38+0100","dateFinished":"2017-12-18T11:34:38+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:31"},{"text":"%kryo-spark\ndef distance(vp1: ValuePoint, vp2: ValuePoint): Int = {\n    if (!vp1.getDate.before(vp2.getDate)) {\n        throw new IllegalArgumentException(\"Arg1 must be before arg2: \" + vp1.toString + \", \" + vp2.toString)\n    }\n    ((vp2.getDate.getTime - vp1.getDate.getTime) / (60000L)).toInt\n}\n\ndef completeMissing(input: java.util.List[ValuePoint]): java.util.List[ValuePoint] = {\n    var inserted = 0\n    val size = input.size\n    for (i <- 1 until size) {\n        // check if 1 minute apart\n        val offset = distance(input.get(i + inserted - 1), input.get(i + inserted)) - 1\n        if (offset > 0 && offset < 60) { // do not complete if more than an our apart\n            val startTime = input.get(i + inserted - 1).getDate.getTime\n            for (j <- 0 until offset) {\n                // Add missing\n                input.add(i + inserted + j, new ValuePoint(-1.0, new Timestamp(startTime + 60000L * (j + 1))))\n            }\n            inserted += offset\n        }\n    }\n    input\n}\n\ndef interpolate(input: Dataset[Counter]): Dataset[Counter] = {\n    input.map(c => {\n        val values = completeMissing(c.getValuePoints)\n        \n        breakable { for (i <- 0 until values.size if values.get(i).getValue == -1.0) {\n            // Optionals for nearby points and their derivatives\n            var last = None: Option[Double]\n            var lastDeriv = None: Option[Double]\n            var next = None: Option[Double]\n            var nextDeriv = None: Option[Double]\n            \n            // Next value that is not minus one\n            var nextNonMissing = values.size\n            \n            // store left bound and derivative, if they exist.\n            if (i > 0) {\n                last = Some(values.get(i - 1).getValue)\n                if (i > 1) {\n                    lastDeriv = Some(last.get - values.get(i - 2).getValue)\n                }\n            }\n            \n            // add right bound and derivative, if they exist\n            (i + 1 until values.size).find(values.get(_).getValue != -1).foreach(n => {\n                next = Option(values.get(n).getValue)\n                nextNonMissing = n\n                \n                (n + 1 until values.size).find(values.get(_).getValue != -1).foreach(nn => {\n                    nextDeriv = Some((values.get(nn).getValue - next.get) / (nn - nextNonMissing))\n                })\n            })\n            \n            // Fail if no value exists \n            if (last.isEmpty && next.isEmpty) {\n                println(\"No valid points to interpolate from for Metric: \" + c.getMetric)\n                c.getValuePoints.clear\n                break\n            } \n            \n            // Heremite Interpolation (used as linear interpolator)\n            val interpolator = new HermiteInterpolator()\n            \n            // Add vlaues to interpolator\n            if ((!lastDeriv.isEmpty && !next.isEmpty) || (!nextDeriv.isEmpty && !last.isEmpty)) {\n                // Check if there is a jump in the curve\n                if (last.get < next.get && nextDeriv.isEmpty && lastDeriv.get < 0) {\n                    // Curve goes down, then jumps up to point --> interpolate point as constant\n                    interpolator.addSamplePoint(nextNonMissing - i, Array(next.get))\n                }\n                else if (last.get < next.get && !nextDeriv.isEmpty && nextDeriv.get < 0 && (lastDeriv.isEmpty || Math.signum(lastDeriv.get) == Math.signum(nextDeriv.get))) {\n                    // Curve jumps upwards, interpolate from right only\n                    interpolator.addSamplePoint(nextNonMissing - i, Array(next.get))//, Array(nextDeriv.get))\n                }\n                else if (last.get > next.get && lastDeriv.isEmpty && nextDeriv.get > 0) {\n                    // Curve jumps down from point then goes up --> interpolate point as constant\n                    interpolator.addSamplePoint(-1, Array(last.get))\n                }\n                else if(last.get > next.get && !lastDeriv.isEmpty && lastDeriv.get > 0 && (nextDeriv.isEmpty || Math.signum(lastDeriv.get) == Math.signum(nextDeriv.get))) {\n                    // Curve jumps downwards, interpolate from left only\n                    interpolator.addSamplePoint(-1, Array(last.get))//, Array(lastDeriv.get))\n                }\n                else {\n                    // No jump, interpolate from all\n                    interpolator.addSamplePoint(-1, Array(last.get))//, Array(lastDeriv.get))\n                    interpolator.addSamplePoint(nextNonMissing - i, Array(next.get))//, Array(nextDeriv.get))\n                }\n            } else {\n                // No jump possible - just add all existing values\n                if (!last.isEmpty) {\n                    interpolator.addSamplePoint(-1, Array(last.get))\n                } else if(nextNonMissing - i > 60) {\n                    interpolator.addSamplePoint(-1, Array(0.0))\n                }\n                if (!next.isEmpty) {\n                    interpolator.addSamplePoint(nextNonMissing - i, Array(next.get))\n                } else if (nextNonMissing - i > 60) {\n                    interpolator.addSamplePoint(nextNonMissing - i, Array(0.0))\n                }\n            }\n            \n            // Interpolate all values in interval 1 until (nex.head - i)\n            for (step <- 0 until (nextNonMissing - i)) {\n                val interpolatedVal = interpolator.value(step)(0)\n                if (interpolatedVal < 0) {\n                    println(\"###### Interpolated value < 0: \" + interpolatedVal + \"! last: \" + last + \" lastderiv: \" + lastDeriv + \" next: \" + next + \" nextDeriv: \" + nextDeriv + \" ########\")\n                }\n                values.get(i + step).setValue(interpolatedVal)\n            }\n        }}\n        \n        c\n    }) (Encoders.bean(classOf[Counter]))\n    .filter(!_.getValuePoints.isEmpty)\n}\n\nobject RangeContext {\n    val start = Timestamp.valueOf(\"2014-12-01 00:00:00\")\n    val end = Timestamp.valueOf(\"2015-9-01 00:00:00\")\n}\n\ndef inRange(input: Dataset[Counter]): Dataset[Counter] = {\n    input.filter(c => {\n        val head = c.getValuePoints.get(0).getDate\n        val tail = c.getValuePoints.get(c.getValuePoints.size - 1).getDate\n        (head.before(RangeContext.start) || head.equals(RangeContext.start)) && (tail.after(RangeContext.end) || tail.equals(RangeContext.end))\n    })\n}","user":"anonymous","dateUpdated":"2017-12-19T11:06:24+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"distance: (vp1: de.qaware.mlwb.api.ValuePoint, vp2: de.qaware.mlwb.api.ValuePoint)Int\ncompleteMissing: (input: java.util.List[de.qaware.mlwb.api.ValuePoint])java.util.List[de.qaware.mlwb.api.ValuePoint]\ninterpolate: (input: org.apache.spark.sql.Dataset[de.qaware.mlwb.api.Counter])org.apache.spark.sql.Dataset[de.qaware.mlwb.api.Counter]\ndefined object RangeContext\ninRange: (input: org.apache.spark.sql.Dataset[de.qaware.mlwb.api.Counter])org.apache.spark.sql.Dataset[de.qaware.mlwb.api.Counter]\n"}]},"apps":[],"jobName":"paragraph_1512404968280_-642059854","id":"20171204-172928_741767776","dateCreated":"2017-12-04T17:29:28+0100","dateStarted":"2017-12-18T11:35:06+0100","dateFinished":"2017-12-18T11:35:11+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:32"},{"text":"%md\n\n# No data","user":"anonymous","dateUpdated":"2017-12-18T11:34:38+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>No data</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1512404836674_518191231","id":"20171204-172716_312816066","dateCreated":"2017-12-04T17:27:16+0100","dateStarted":"2017-12-18T11:34:39+0100","dateFinished":"2017-12-18T11:34:39+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33"},{"text":"%kryo-spark\nval start = Timestamp.valueOf(\"2014-11-29 00:00:00\")\nval end = Timestamp.valueOf(\"2015-10-03 00:00:00\")\n\nval readService = new MetricsServiceImpl.Factory(sqlc).getInstance(\"192.168.1.100:2181\", \"ekgdata\")\nval query = new QueryMetricContext.Builder().withMetrics(\"*Runtime*Uptime*\").withHost(\"lp*\").withStart(start).withEnd(end).build()\n\nval minusOne = readService.getCounters(query, Granularity.MINUTE, AggregationType.MIN).map(c => {\n    c.getMetric.setName(\"No data\")\n    val consumer = new Consumer[ValuePoint]() {\n        override def accept(v: ValuePoint): Unit = {\n            v.setValue(if (v.getValue == -1) 1.0 else 0.0)\n        }\n    }\n    completeMissing(c.getValuePoints).forEach(consumer)\n    c\n}) (Encoders.bean(classOf[Counter]))","user":"anonymous","dateUpdated":"2017-12-18T11:34:39+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"start: java.sql.Timestamp = 2014-11-29 00:00:00.0\nend: java.sql.Timestamp = 2015-10-03 00:00:00.0\nreadService: de.qaware.mlwb.api.MetricsService = de.qaware.mlwb.impl.sparksolr.MetricsServiceImpl@6d26f403\nquery: de.qaware.mlwb.api.QueryMetricContext = de.qaware.mlwb.api.QueryMetricContext@104d437d[series=*,host=lp*,process=*,start=2014-11-29 00:00:00.0,end=2015-10-03 00:00:00.0,rawQuery=<null>]\nminusOne: org.apache.spark.sql.Dataset[de.qaware.mlwb.api.Counter] = [metric: struct<host: string, name: string ... 1 more field>, valuePoints: array<struct<date:timestamp,value:double>>]\n"}]},"apps":[],"jobName":"paragraph_1512404818354_-479508639","id":"20171204-172658_753176101","dateCreated":"2017-12-04T17:26:58+0100","dateStarted":"2017-12-18T11:35:10+0100","dateFinished":"2017-12-18T11:35:13+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:34"},{"text":"%md\n# Methods for feature creation","user":"anonymous","dateUpdated":"2017-12-18T11:34:39+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Methods for feature creation</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1508831844123_1097498819","id":"20171024-095724_1875950622","dateCreated":"2017-10-24T09:57:24+0200","dateStarted":"2017-12-18T11:34:39+0100","dateFinished":"2017-12-18T11:34:39+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:35"},{"text":"%kryo-spark\ndef combFun(comb: Combinator) = comb match {\n    case Addition => ((_: Double) + (_: Double))\n    case Substraction => ((_: Double) - (_: Double))\n    case Multiplication => ((_: Double) * (_: Double))\n    case Division => ((l: Double, r: Double) => if (r == 0.0) 1.0 else l / r)\n}\n\ndef combString(comb: Combinator) = comb match {\n    case Addition => \" + \"\n    case Substraction => \" - \"\n    case Multiplication => \" * \"\n    case Division => \" / \"\n}\n\ndef leftCombine(left: Dataset[Counter], right: Double, comb: Combinator): Dataset[Counter] = {\n    left.map(c => {\n        c.getValuePoints.asScala.foreach(vp => vp.setValue(combFun(comb)(vp.getValue, right)))\n        c.getMetric.setName(\"(\" + c.getMetric.getName + combString(comb) + right + \")\")\n        c\n    }) (Encoders.bean(classOf[Counter]))\n}\n\ndef  rightCombine(left: Double, right: Dataset[Counter], comb: Combinator): Dataset[Counter] = {\n    right.map(c => {\n        c.getValuePoints.asScala.foreach(vp => vp.setValue(combFun(comb)(left, vp.getValue)))\n        c.getMetric.setName(\"(\" + left + combString(comb) + c.getMetric.getName + \")\")\n        c\n    }) (Encoders.bean(classOf[Counter]))\n}\n\ndef biCombine(left: Dataset[Counter], right: Dataset[Counter], comb: Combinator): Dataset[Counter] = {\n    val fun = combFun(comb)\n    \n    left.joinWith(right, (right.col(\"metric.host\") === left.col(\"metric.host\")).and(right.col(\"metric.procs\") === left.col(\"metric.procs\"))).map(t => {\n        val itl = Iterators.peekingIterator(t._1.getValuePoints.iterator)\n        val itr = Iterators.peekingIterator(t._2.getValuePoints.iterator)\n        \n        val c = new Counter(new Metric(\"(\" + t._1.getMetric.getName + combString(comb) + t._2.getMetric.getName + \")\", t._1.getMetric.getHost, t._1.getMetric.getProcs))\n        while (itl.hasNext && itr.hasNext) {\n            breakable {\n                // continue loop until both iterators are on same date\n                if (itl.peek.getDate != itr.peek.getDate) {\n                    if (itl.peek.getDate.before(itr.peek.getDate)) itl.next else itr.next\n                    break\n                }\n                // Create new point at that date\n                val left = itl.next\n                c.getValuePoints.add(new ValuePoint(fun(left.getValue, itr.next.getValue), left.getDate))\n            }\n        }\n        c\n    }) (Encoders.bean(classOf[Counter]))\n    // If the date ranges do not match, result could be empty\n    .filter(!_.getValuePoints.isEmpty)\n}\n\ndef derivePoint(previous: ValuePoint, point: ValuePoint): ValuePoint = {\n    new ValuePoint((point.getValue - previous.getValue) / ((point.getDate.getTime - previous.getDate.getTime) / 60000L), point.getDate)\n}\n\ndef derive(data: Dataset[Counter]): Dataset[Counter] = {\n    data.map(c => {\n        val deriv = new Counter(new Metric(\"d/dx (\" + c.getMetric.getName + \")\", c.getMetric.getHost, c.getMetric.getProcs))\n        val vp = c.getValuePoints.iterator\n        \n        // Create derivative for first two points (first point gets same derivative as second point)\n        val first = vp.next\n        val second = vp.next\n        deriv.getValuePoints.add(new ValuePoint(derivePoint(first, second).getValue, first.getDate))\n        deriv.getValuePoints.add(derivePoint(first, second))\n        \n        // Create derivative for all remaining\n        var previous = second\n        while (vp.hasNext) {\n            val point = vp.next\n            deriv.getValuePoints.add(derivePoint(previous, point))\n            previous = point\n        }\n        deriv\n    }) (Encoders.bean(classOf[Counter]))\n}\n\ndef max(data: Dataset[Counter], inp: Double): Dataset[Counter] = {\n    data.map(c => {\n        c.getValuePoints.asScala.foreach(vp => if (vp.getValue < inp) vp.setValue(inp))\n        c.getMetric.setName(\"incl\" + c.getMetric.getName)\n        c\n    }) (Encoders.bean(classOf[Counter]))\n}\n\ndef stripProcessId(c: Counter): Counter = {\n    val key = \"\\\\Process(java\"\n    val name = c.getMetric.getName\n    \n    if (name.contains(key) && c.getMetric.getProcs == \"wls2\") {\n        c.getMetric.setName(name.replace(\"#1\", \"\"))\n    }\n    return c\n}\n\ndef stripServerRuntimes(c: Counter): Counter = {\n    val key = \"ServerRuntime\"\n    val name = c.getMetric.getName\n    val host = c.getMetric.getHost\n    \n    if (name.contains(host) && name.contains(key) &&  name.indexOf(key) < name.indexOf(host)) {\n        val strippedName = (name.substring(0, name.indexOf(key)) + name.substring(name.indexOf(host) + host.length, name.length)).replace(\",,\", \",\")\n        c.getMetric.setName(strippedName)\n    }\n    return c\n}\n\ndef stripNames(data: Dataset[Counter]): Dataset[Counter] = {\n    data.map(c => stripServerRuntimes(stripProcessId(c))) (Encoders.bean(classOf[Counter]))\n}\n\ndef nameMetric(name: String, data: Dataset[Counter]): Dataset[Counter] = {\n    data.map(c => {\n        c.getMetric.setName(name + \" : (\" + c.getMetric.getName + \")\")\n        c\n    }) (Encoders.bean(classOf[Counter]))\n}","user":"anonymous","dateUpdated":"2017-12-19T11:06:36+0100","config":{"tableHide":true,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"<console>:55: warning: match may not be exhaustive.\nIt would fail on the following inputs: Addition, Division, Multiplication, Substraction\n       def combFun(comb: Combinator) = comb match {\n                                       ^\ncombFun: (comb: Combinator)(Double, Double) => Double\n<console>:56: warning: match may not be exhaustive.\nIt would fail on the following inputs: Addition, Division, Multiplication, Substraction\n       def combString(comb: Combinator) = comb match {\n                                          ^\ncombString: (comb: Combinator)String\nleftCombine: (left: org.apache.spark.sql.Dataset[de.qaware.mlwb.api.Counter], right: Double, comb: Combinator)org.apache.spark.sql.Dataset[de.qaware.mlwb.api.Counter]\nrightCombine: (left: Double, right: org.apache.spark.sql.Dataset[de.qaware.mlwb.api.Counter], comb: Combinator)org.apache.spark.sql.Dataset[de.qaware.mlwb.api.Counter]\nbiCombine: (left: org.apache.spark.sql.Dataset[de.qaware.mlwb.api.Counter], right: org.apache.spark.sql.Dataset[de.qaware.mlwb.api.Counter], comb: Combinator)org.apache.spark.sql.Dataset[de.qaware.mlwb.api.Counter]\nderivePoint: (previous: de.qaware.mlwb.api.ValuePoint, point: de.qaware.mlwb.api.ValuePoint)de.qaware.mlwb.api.ValuePoint\nderive: (data: org.apache.spark.sql.Dataset[de.qaware.mlwb.api.Counter])org.apache.spark.sql.Dataset[de.qaware.mlwb.api.Counter]\nmax: (data: org.apache.spark.sql.Dataset[de.qaware.mlwb.api.Counter], inp: Double)org.apache.spark.sql.Dataset[de.qaware.mlwb.api.Counter]\nstripProcessId: (c: de.qaware.mlwb.api.Counter)de.qaware.mlwb.api.Counter\nstripServerRuntimes: (c: de.qaware.mlwb.api.Counter)de.qaware.mlwb.api.Counter\nstripNames: (data: org.apache.spark.sql.Dataset[de.qaware.mlwb.api.Counter])org.apache.spark.sql.Dataset[de.qaware.mlwb.api.Counter]\nnameMetric: (name: String, data: org.apache.spark.sql.Dataset[de.qaware.mlwb.api.Counter])org.apache.spark.sql.Dataset[de.qaware.mlwb.api.Counter]\n"}]},"apps":[],"jobName":"paragraph_1505309655708_-317399219","id":"20170721-154212_1747862524","dateCreated":"2017-09-13T15:34:15+0200","dateStarted":"2017-12-18T11:35:11+0100","dateFinished":"2017-12-18T11:35:16+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:36"},{"text":"%kryo-spark\ndef processFeature(feature: Feature, input: Map[MetricKey, Dataset[Counter]], inTf: Dataset[Counter] => Dataset[Counter]): Dataset[Counter] = {\n    feature match {\n        case m : MetricKey => stripNames(inTf(input.get(m).get))\n        case NamedFeature(n, f) => nameMetric(n, processFeature(f, input, inTf))\n        case Derivative(f) => derive(processFeature(f, input, inTf))\n        case Inclination(f) => max(derive(processFeature(f, input, inTf)), 0.0)\n        case BiCombination(f1, f2, comb) => biCombine(processFeature(f1, input, inTf), processFeature(f2, input, inTf), comb)\n        case LeftCombination(f, const, comb) => leftCombine(processFeature(f, input, inTf), const, comb)\n        case RightCombination(const, f, comb) => rightCombine(const, processFeature(f, input, inTf), comb)\n    }\n}\n\ndef processFeatures(features: Set[Feature], service: MetricsService, start: Timestamp, end: Timestamp): Dataset[Counter] = {\n    val inputData = features.flatMap(extractMetrics(_))\n        .map(m => m -> service.getCounters(new QueryMetricContext.Builder().withMetrics(m.toQueryString).withHost(\"lp*\").withStart(start).withEnd(end).build(), Granularity.MINUTE, AggregationType.MIN))\n        .toMap\n    \n    features\n        .map(processFeature(_, inputData, (x => interpolate(inRange(x.filter(!_.getMetric.getName.contains(\"weblogic.jms.JMSBatch\")))))))\n        .reduce(_.union(_))\n}","user":"anonymous","dateUpdated":"2017-12-18T11:34:39+0100","config":{"tableHide":true,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{"0":{"graph":{"mode":"table","height":251.8,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"<console>:96: warning: match may not be exhaustive.\nIt would fail on the following inputs: BiCombination(_, _, _), Derivative(_), Inclination(_), LeftCombination(_, _, _), MetricKey(_, _), NamedFeature(_, _), RightCombination(_, _, _)\n           feature match {\n           ^\nprocessFeature: (feature: Feature, input: Map[MetricKey,org.apache.spark.sql.Dataset[de.qaware.mlwb.api.Counter]], inTf: org.apache.spark.sql.Dataset[de.qaware.mlwb.api.Counter] => org.apache.spark.sql.Dataset[de.qaware.mlwb.api.Counter])org.apache.spark.sql.Dataset[de.qaware.mlwb.api.Counter]\nprocessFeatures: (features: Set[Feature], service: de.qaware.mlwb.api.MetricsService, start: java.sql.Timestamp, end: java.sql.Timestamp)org.apache.spark.sql.Dataset[de.qaware.mlwb.api.Counter]\n"}]},"apps":[],"jobName":"paragraph_1505309655709_-317783968","id":"20170706-112034_348519821","dateCreated":"2017-09-13T15:34:15+0200","dateStarted":"2017-12-18T11:35:14+0100","dateFinished":"2017-12-18T11:35:16+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37"},{"text":"%md\n\n# List of all features","user":"anonymous","dateUpdated":"2017-12-18T11:34:39+0100","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>List of all features</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1505309655709_-317783968","id":"20170720-142101_1554954686","dateCreated":"2017-09-13T15:34:15+0200","dateStarted":"2017-12-18T11:34:39+0100","dateFinished":"2017-12-18T11:34:39+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38"},{"text":"%kryo-spark\ndef explodeBiComb(l: String, r: String, keys: List[String], values: List[List[String]], c: Combinator): Set[Feature] = {\n    values.map(s => keys.zip(s).map(t => t._1 + \"=\" + t._2).toSet).map(s => BiCombination(MetricKey(l, s), MetricKey(r, s), c)).toSet\n}\n\nval datasources = \"source10\" :: (1 to 9).map(\"source0\" + _).toList\n\nvar features = Set[Feature]()\n\nfeatures ++= Set[Feature](\n    NamedFeature(\"Class loading activity\", Derivative(MetricKey(\".LoadedClassCount\", Set(\"java.lang:type\")))),\n    NamedFeature(\"Class unloading activity\", Derivative(MetricKey(\".UnloadedClassCount\", Set(\"java.lang:type\")))),\n    NamedFeature(\"Rel. physical mem usage\", BiCombination(MetricKey(\".FreePhysicalMemorySize\", Set(\"java.lang:type\")), MetricKey(\".TotalPhysicalMemorySize\", Set(\"java.lang:type\")), Division)),\n    NamedFeature(\"Physical mem activity\", Derivative(MetricKey(\".FreePhysicalMemorySize\", Set(\"java.lang:type\")))),\n    NamedFeature(\"Rel. swap usage\", BiCombination(MetricKey(\".FreeSwapSpaceSize\", Set(\"java.lang:type\")), MetricKey(\".TotalSwapSpaceSize\", Set(\"java.lang:type\")), Division)),\n    NamedFeature(\"Swap activity\", Derivative(MetricKey(\".FreeSwapSpaceSize\", Set(\"java.lang:type\")))),\n    NamedFeature(\"Rel. open file descriptors\", BiCombination(MetricKey(\".OpenFileDescriptorCount\", Set(\"java.lang:type\")), MetricKey(\".MaxFileDescriptorCount\", Set(\"java.lang:type\")), Division)),\n    NamedFeature(\"Process CPU\", MetricKey(\".ProcessCpuLoad\", Set(\"java.lang:type\"))),\n    NamedFeature(\"System CPU\", MetricKey(\".SystemCpuLoad\", Set(\"java.lang:type\"))),\n    NamedFeature(\"Failing reserve requests\", Inclination(MetricKey(\".FailedReserveRequestCount\", Set(\"com.bea:Name\", \"Type\")))),\n    NamedFeature(\"Reserve request activity\", Inclination(MetricKey(\".ReserveRequestCount\", Set(\"com.bea:Name\", \"Type\"))))\n)\n\n//new\nfeatures ++= explodeBiComb(\".PrepStmtCacheHitCount\", \".PrepStmtCacheMissCount\", List(\"com.bea:Name\", \"Type\"), datasources.map(List(_, \"*\")), Division).map(NamedFeature(\"Prepared statement cache hit rate\", _))\n\nfeatures ++= Set(\n    NamedFeature(\"Successful wait for connection\", Inclination(MetricKey(\".WaitingForConnectionSuccessTotal\", Set(\"com.bea:Name\", \"Type\")))),\n    NamedFeature(\"Failed wait for connection\", Inclination(MetricKey(\".WaitingForConnectionFailureTotal\", Set(\"com.bea:Name\", \"Type\")))),\n    NamedFeature(\"Rel. heap usage\", BiCombination(MetricKey(\".HeapMemoryUsage.used\", Set(\"java.lang:type\")), MetricKey(\".HeapMemoryUsage.max\", Set(\"java.lang:type\")), Division)),\n    NamedFeature(\"Rel. heap committed\", BiCombination(MetricKey(\".HeapMemoryUsage.committed\", Set(\"java.lang:type\")), MetricKey(\".HeapMemoryUsage.max\", Set(\"java.lang:type\")), Division)),\n    NamedFeature(\"Heap usage activity\", Derivative(MetricKey(\".HeapMemoryUsage.used\", Set(\"java.lang:type\")))),\n    NamedFeature(\"Heap committed activity\", Derivative(MetricKey(\".HeapMemoryUsage.committed\", Set(\"java.lang:type\")))),\n    NamedFeature(\"Rel. nonHeap usage\", BiCombination(MetricKey(\".NonHeapMemoryUsage.used\", Set(\"java.lang:type\")), MetricKey(\".NonHeapMemoryUsage.max\", Set(\"java.lang:type\")), Division)),\n    NamedFeature(\"Rel. nonHeap committed\", BiCombination(MetricKey(\".NonHeapMemoryUsage.committed\", Set(\"java.lang:type\")), MetricKey(\".NonHeapMemoryUsage.max\", Set(\"java.lang:type\")), Division)),\n    NamedFeature(\"NonHeap usage activity\", Derivative(MetricKey(\".NonHeapMemoryUsage.used\", Set(\"java.lang:type\")))),\n    NamedFeature(\"NonHeap committed activity\", Derivative(MetricKey(\".NonHeapMemoryUsage.committed\", Set(\"java.lang:type\")))),\n    NamedFeature(\"Objects to be finalized\", MetricKey(\".ObjectPendingFinalizationCount\", Set(\"java.lang:type\")))\n)\n\nfeatures ++= List(\".Usage.used\", \".Usage.committed\").flatMap(explodeBiComb(_, \".Usage.max\", List(\"java.lang:name\", \"type\"), List(\n    \"Code*Cache\", \n    \"PS*Eden*Space\", \n    \"PS*Old*Gen\", \n    \"PS*Perm*Gen\", \n    \"PS*Survivor*Space\")\n    .map(List(_, \"*\")), Division)).flatMap(x => Iterator(NamedFeature(\"Memory space usage\", x), NamedFeature(\"Memory space activity\", Derivative(x))))\n\nfeatures ++= Set[Feature](\n    NamedFeature(\"Rel. Swap Usage\", BiCombination(MetricKey(\"\\\\Swap\\\\used\", Set()), MetricKey(\"\\\\Swap\\\\total\", Set()), Division)),\n    NamedFeature(\"Stuck threads\", MetricKey(\".StuckThreadCount\", Set(\"com.bea:ApplicationRuntime\", \"Name\", \"Type\"))),\n    NamedFeature(\"Thread CPU time\", MetricKey(\"CurrentThreadCpuTime\", Set(\"java.lang:type\"))),\n    NamedFeature(\"Thread User time\", MetricKey(\"CurrentThreadUserTime\", Set(\"java.lang:type\"))),\n    NamedFeature(\"Daemon thread count\", MetricKey(\".DaemonThreadCount\", Set(\"java.lang:type\"))),\n    NamedFeature(\"Total thread count\", MetricKey(\".ThreadCount\", Set(\"java.lang:type\"))),\n    NamedFeature(\"Active transactions\", MetricKey(\".ActiveTransactionsTotalCount\", Set(\"com.bea:Name\", \"Type\"))),\n    NamedFeature(\"Transaction committ activity\", Inclination(MetricKey(\".TransactionCommittedTotalCount\", Set(\"com.bea:Name\", \"Type\")))),\n    NamedFeature(\"Transaction roll back activity\", Inclination(MetricKey(\".TransactionRolledBackTotalCount\", Set(\"com.bea:Name\", \"Type\")))),\n    NamedFeature(\"Active connections\", MetricKey(\".ActiveConnectionsCurrentCount\", Set(\"com.bea:Name\", \"Type\"))),\n    NamedFeature(\"Connection delay\", MetricKey(\".ConnectionDelayTime\", Set(\"com.bea:Name\", \"Type\"))),\n    NamedFeature(\"DB connection started\", Inclination(MetricKey(\".ConnectionsTotalCount\", Set(\"com.bea:Name\", \"Type\")))),\n    NamedFeature(\"Available db connection activity\", Derivative(MetricKey(\".NumAvailable\", Set(\"com.bea:Name\", \"Type\"))))\n)\n//new\nfeatures ++= explodeBiComb(\".NumUnavailable\", \".CurrCapacity\", List(\"com.bea:Name\", \"Type\"), \n    datasources.flatMap(d => Seq(List(d, \"JDBCDataSourceRuntime\"), List(d, \"JDBCConnectionPoolRuntime\"))), Division)\n    .map(NamedFeature(\"Rel. unavailable connections\", _))\n    \nfeatures ++= Set(\n    NamedFeature(\"Process CPU\", MetricKey(\"\\\\Process(java*)\\\\CPU\", Set())),\n    NamedFeature(\"Stuck threads\", MetricKey(\".StuckThreadCount\", Set(\"com.bea:Name\", \"Type\"))),\n    NamedFeature(\"GC activity\", Inclination(MetricKey(\".CollectionCount\", Set(\"java.lang:name\", \"type\")))),\n    NamedFeature(\"GC time\", Inclination(MetricKey(\".CollectionTime\", Set(\"java.lang:name\", \"type\")))),\n    NamedFeature(\"Last GC duration\", MetricKey(\".LastGcInfo.duration\", Set(\"java.lang:name\", \"type\")))\n)\nfeatures.foreach(println(_))\nfeatures.size","user":"anonymous","dateUpdated":"2018-01-11T09:49:44+0100","config":{"lineNumbers":true,"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{"0":{"graph":{"mode":"table","height":358,"optionOpen":false}}},"enabled":true,"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"explodeBiComb: (l: String, r: String, keys: List[String], values: List[List[String]], c: Combinator)Set[Feature]\ndatasources: List[String] = List(source10, source01, source02, source03, source04, source05, source06, source07, source08, source09)\nfeatures: scala.collection.immutable.Set[Feature] = Set()\nNamedFeature(Total thread count,MetricKey(.ThreadCount,Set(java.lang:type)))\nNamedFeature(GC activity,Inclination(MetricKey(.CollectionCount,Set(java.lang:name, type))))\nNamedFeature(Transaction committ activity,Inclination(MetricKey(.TransactionCommittedTotalCount,Set(com.bea:Name, Type))))\nNamedFeature(Prepared statement cache hit rate,BiCombination(MetricKey(.PrepStmtCacheHitCount,Set(com.bea:Name=source08, Type=*)),MetricKey(.PrepStmtCacheMissCount,Set(com.bea:Name=source08, Type=*)),Division))\nNamedFeature(Rel. unavailable connections,BiCombination(MetricKey(.NumUnavailable,Set(com.bea:Name=source08, Type=JDBCDataSourceRuntime)),MetricKey(.CurrCapacity,Set(com.bea:Name=source08, Type=JDBCDataSourceRuntime)),Division))\nNamedFeature(Memory space activity,Derivative(BiCombination(MetricKey(.Usage.used,Set(java.lang:name=PS*Eden*Space, type=*)),MetricKey(.Usage.max,Set(java.lang:name=PS*Eden*Space, type=*)),Division)))\nNamedFeature(Heap usage activity,Derivative(MetricKey(.HeapMemoryUsage.used,Set(java.lang:type))))\nNamedFeature(Active connections,MetricKey(.ActiveConnectionsCurrentCount,Set(com.bea:Name, Type)))\nNamedFeature(Memory space usage,BiCombination(MetricKey(.Usage.used,Set(java.lang:name=PS*Perm*Gen, type=*)),MetricKey(.Usage.max,Set(java.lang:name=PS*Perm*Gen, type=*)),Division))\nNamedFeature(Heap committed activity,Derivative(MetricKey(.HeapMemoryUsage.committed,Set(java.lang:type))))\nNamedFeature(Memory space activity,Derivative(BiCombination(MetricKey(.Usage.committed,Set(java.lang:name=PS*Survivor*Space, type=*)),MetricKey(.Usage.max,Set(java.lang:name=PS*Survivor*Space, type=*)),Division)))\nNamedFeature(Process CPU,MetricKey(.ProcessCpuLoad,Set(java.lang:type)))\nNamedFeature(Stuck threads,MetricKey(.StuckThreadCount,Set(com.bea:Name, Type)))\nNamedFeature(GC time,Inclination(MetricKey(.CollectionTime,Set(java.lang:name, type))))\nNamedFeature(Memory space activity,Derivative(BiCombination(MetricKey(.Usage.used,Set(java.lang:name=PS*Old*Gen, type=*)),MetricKey(.Usage.max,Set(java.lang:name=PS*Old*Gen, type=*)),Division)))\nNamedFeature(Rel. unavailable connections,BiCombination(MetricKey(.NumUnavailable,Set(com.bea:Name=source03, Type=JDBCConnectionPoolRuntime)),MetricKey(.CurrCapacity,Set(com.bea:Name=source03, Type=JDBCConnectionPoolRuntime)),Division))\nNamedFeature(Last GC duration,MetricKey(.LastGcInfo.duration,Set(java.lang:name, type)))\nNamedFeature(Rel. swap usage,BiCombination(MetricKey(.FreeSwapSpaceSize,Set(java.lang:type)),MetricKey(.TotalSwapSpaceSize,Set(java.lang:type)),Division))\nNamedFeature(Memory space usage,BiCombination(MetricKey(.Usage.committed,Set(java.lang:name=PS*Perm*Gen, type=*)),MetricKey(.Usage.max,Set(java.lang:name=PS*Perm*Gen, type=*)),Division))\nNamedFeature(Memory space activity,Derivative(BiCombination(MetricKey(.Usage.used,Set(java.lang:name=PS*Perm*Gen, type=*)),MetricKey(.Usage.max,Set(java.lang:name=PS*Perm*Gen, type=*)),Division)))\nNamedFeature(Prepared statement cache hit rate,BiCombination(MetricKey(.PrepStmtCacheHitCount,Set(com.bea:Name=source06, Type=*)),MetricKey(.PrepStmtCacheMissCount,Set(com.bea:Name=source06, Type=*)),Division))\nNamedFeature(Rel. unavailable connections,BiCombination(MetricKey(.NumUnavailable,Set(com.bea:Name=source04, Type=JDBCDataSourceRuntime)),MetricKey(.CurrCapacity,Set(com.bea:Name=source04, Type=JDBCDataSourceRuntime)),Division))\nNamedFeature(Memory space usage,BiCombination(MetricKey(.Usage.committed,Set(java.lang:name=Code*Cache, type=*)),MetricKey(.Usage.max,Set(java.lang:name=Code*Cache, type=*)),Division))\nNamedFeature(Memory space activity,Derivative(BiCombination(MetricKey(.Usage.used,Set(java.lang:name=Code*Cache, type=*)),MetricKey(.Usage.max,Set(java.lang:name=Code*Cache, type=*)),Division)))\nNamedFeature(Memory space usage,BiCombination(MetricKey(.Usage.committed,Set(java.lang:name=PS*Survivor*Space, type=*)),MetricKey(.Usage.max,Set(java.lang:name=PS*Survivor*Space, type=*)),Division))\nNamedFeature(Rel. nonHeap usage,BiCombination(MetricKey(.NonHeapMemoryUsage.used,Set(java.lang:type)),MetricKey(.NonHeapMemoryUsage.max,Set(java.lang:type)),Division))\nNamedFeature(Rel. unavailable connections,BiCombination(MetricKey(.NumUnavailable,Set(com.bea:Name=source10, Type=JDBCDataSourceRuntime)),MetricKey(.CurrCapacity,Set(com.bea:Name=source10, Type=JDBCDataSourceRuntime)),Division))\nNamedFeature(Rel. heap committed,BiCombination(MetricKey(.HeapMemoryUsage.committed,Set(java.lang:type)),MetricKey(.HeapMemoryUsage.max,Set(java.lang:type)),Division))\nNamedFeature(Rel. unavailable connections,BiCombination(MetricKey(.NumUnavailable,Set(com.bea:Name=source05, Type=JDBCDataSourceRuntime)),MetricKey(.CurrCapacity,Set(com.bea:Name=source05, Type=JDBCDataSourceRuntime)),Division))\nNamedFeature(Rel. unavailable connections,BiCombination(MetricKey(.NumUnavailable,Set(com.bea:Name=source09, Type=JDBCConnectionPoolRuntime)),MetricKey(.CurrCapacity,Set(com.bea:Name=source09, Type=JDBCConnectionPoolRuntime)),Division))\nNamedFeature(Thread User time,MetricKey(CurrentThreadUserTime,Set(java.lang:type)))\nNamedFeature(Rel. unavailable connections,BiCombination(MetricKey(.NumUnavailable,Set(com.bea:Name=source06, Type=JDBCConnectionPoolRuntime)),MetricKey(.CurrCapacity,Set(com.bea:Name=source06, Type=JDBCConnectionPoolRuntime)),Division))\nNamedFeature(Prepared statement cache hit rate,BiCombination(MetricKey(.PrepStmtCacheHitCount,Set(com.bea:Name=source05, Type=*)),MetricKey(.PrepStmtCacheMissCount,Set(com.bea:Name=source05, Type=*)),Division))\nNamedFeature(Failed wait for connection,Inclination(MetricKey(.WaitingForConnectionFailureTotal,Set(com.bea:Name, Type))))\nNamedFeature(Memory space activity,Derivative(BiCombination(MetricKey(.Usage.used,Set(java.lang:name=PS*Survivor*Space, type=*)),MetricKey(.Usage.max,Set(java.lang:name=PS*Survivor*Space, type=*)),Division)))\nNamedFeature(Rel. unavailable connections,BiCombination(MetricKey(.NumUnavailable,Set(com.bea:Name=source06, Type=JDBCDataSourceRuntime)),MetricKey(.CurrCapacity,Set(com.bea:Name=source06, Type=JDBCDataSourceRuntime)),Division))\nNamedFeature(Memory space activity,Derivative(BiCombination(MetricKey(.Usage.committed,Set(java.lang:name=PS*Old*Gen, type=*)),MetricKey(.Usage.max,Set(java.lang:name=PS*Old*Gen, type=*)),Division)))\nNamedFeature(Rel. unavailable connections,BiCombination(MetricKey(.NumUnavailable,Set(com.bea:Name=source05, Type=JDBCConnectionPoolRuntime)),MetricKey(.CurrCapacity,Set(com.bea:Name=source05, Type=JDBCConnectionPoolRuntime)),Division))\nNamedFeature(Daemon thread count,MetricKey(.DaemonThreadCount,Set(java.lang:type)))\nNamedFeature(DB connection started,Inclination(MetricKey(.ConnectionsTotalCount,Set(com.bea:Name, Type))))\nNamedFeature(Available db connection activity,Derivative(MetricKey(.NumAvailable,Set(com.bea:Name, Type))))\nNamedFeature(Memory space usage,BiCombination(MetricKey(.Usage.used,Set(java.lang:name=PS*Eden*Space, type=*)),MetricKey(.Usage.max,Set(java.lang:name=PS*Eden*Space, type=*)),Division))\nNamedFeature(Rel. unavailable connections,BiCombination(MetricKey(.NumUnavailable,Set(com.bea:Name=source10, Type=JDBCConnectionPoolRuntime)),MetricKey(.CurrCapacity,Set(com.bea:Name=source10, Type=JDBCConnectionPoolRuntime)),Division))\nNamedFeature(Prepared statement cache hit rate,BiCombination(MetricKey(.PrepStmtCacheHitCount,Set(com.bea:Name=source02, Type=*)),MetricKey(.PrepStmtCacheMissCount,Set(com.bea:Name=source02, Type=*)),Division))\nNamedFeature(Rel. open file descriptors,BiCombination(MetricKey(.OpenFileDescriptorCount,Set(java.lang:type)),MetricKey(.MaxFileDescriptorCount,Set(java.lang:type)),Division))\nNamedFeature(Memory space usage,BiCombination(MetricKey(.Usage.used,Set(java.lang:name=Code*Cache, type=*)),MetricKey(.Usage.max,Set(java.lang:name=Code*Cache, type=*)),Division))\nNamedFeature(Prepared statement cache hit rate,BiCombination(MetricKey(.PrepStmtCacheHitCount,Set(com.bea:Name=source10, Type=*)),MetricKey(.PrepStmtCacheMissCount,Set(com.bea:Name=source10, Type=*)),Division))\nNamedFeature(Rel. unavailable connections,BiCombination(MetricKey(.NumUnavailable,Set(com.bea:Name=source09, Type=JDBCDataSourceRuntime)),MetricKey(.CurrCapacity,Set(com.bea:Name=source09, Type=JDBCDataSourceRuntime)),Division))\nNamedFeature(Connection delay,MetricKey(.ConnectionDelayTime,Set(com.bea:Name, Type)))\nNamedFeature(Memory space usage,BiCombination(MetricKey(.Usage.committed,Set(java.lang:name=PS*Eden*Space, type=*)),MetricKey(.Usage.max,Set(java.lang:name=PS*Eden*Space, type=*)),Division))\nNamedFeature(Transaction roll back activity,Inclination(MetricKey(.TransactionRolledBackTotalCount,Set(com.bea:Name, Type))))\nNamedFeature(Active transactions,MetricKey(.ActiveTransactionsTotalCount,Set(com.bea:Name, Type)))\nNamedFeature(Memory space usage,BiCombination(MetricKey(.Usage.used,Set(java.lang:name=PS*Survivor*Space, type=*)),MetricKey(.Usage.max,Set(java.lang:name=PS*Survivor*Space, type=*)),Division))\nNamedFeature(Rel. unavailable connections,BiCombination(MetricKey(.NumUnavailable,Set(com.bea:Name=source08, Type=JDBCConnectionPoolRuntime)),MetricKey(.CurrCapacity,Set(com.bea:Name=source08, Type=JDBCConnectionPoolRuntime)),Division))\nNamedFeature(Reserve request activity,Inclination(MetricKey(.ReserveRequestCount,Set(com.bea:Name, Type))))\nNamedFeature(Thread CPU time,MetricKey(CurrentThreadCpuTime,Set(java.lang:type)))\nNamedFeature(Failing reserve requests,Inclination(MetricKey(.FailedReserveRequestCount,Set(com.bea:Name, Type))))\nNamedFeature(Rel. unavailable connections,BiCombination(MetricKey(.NumUnavailable,Set(com.bea:Name=source02, Type=JDBCConnectionPoolRuntime)),MetricKey(.CurrCapacity,Set(com.bea:Name=source02, Type=JDBCConnectionPoolRuntime)),Division))\nNamedFeature(Rel. heap usage,BiCombination(MetricKey(.HeapMemoryUsage.used,Set(java.lang:type)),MetricKey(.HeapMemoryUsage.max,Set(java.lang:type)),Division))\nNamedFeature(Process CPU,MetricKey(\\Process(java*)\\CPU,Set()))\nNamedFeature(System CPU,MetricKey(.SystemCpuLoad,Set(java.lang:type)))\nNamedFeature(Rel. unavailable connections,BiCombination(MetricKey(.NumUnavailable,Set(com.bea:Name=source07, Type=JDBCConnectionPoolRuntime)),MetricKey(.CurrCapacity,Set(com.bea:Name=source07, Type=JDBCConnectionPoolRuntime)),Division))\nNamedFeature(NonHeap committed activity,Derivative(MetricKey(.NonHeapMemoryUsage.committed,Set(java.lang:type))))\nNamedFeature(Prepared statement cache hit rate,BiCombination(MetricKey(.PrepStmtCacheHitCount,Set(com.bea:Name=source03, Type=*)),MetricKey(.PrepStmtCacheMissCount,Set(com.bea:Name=source03, Type=*)),Division))\nNamedFeature(NonHeap usage activity,Derivative(MetricKey(.NonHeapMemoryUsage.used,Set(java.lang:type))))\nNamedFeature(Rel. nonHeap committed,BiCombination(MetricKey(.NonHeapMemoryUsage.committed,Set(java.lang:type)),MetricKey(.NonHeapMemoryUsage.max,Set(java.lang:type)),Division))\nNamedFeature(Rel. Swap Usage,BiCombination(MetricKey(\\Swap\\used,Set()),MetricKey(\\Swap\\total,Set()),Division))\nNamedFeature(Memory space usage,BiCombination(MetricKey(.Usage.committed,Set(java.lang:name=PS*Old*Gen, type=*)),MetricKey(.Usage.max,Set(java.lang:name=PS*Old*Gen, type=*)),Division))\nNamedFeature(Rel. physical mem usage,BiCombination(MetricKey(.FreePhysicalMemorySize,Set(java.lang:type)),MetricKey(.TotalPhysicalMemorySize,Set(java.lang:type)),Division))\nNamedFeature(Prepared statement cache hit rate,BiCombination(MetricKey(.PrepStmtCacheHitCount,Set(com.bea:Name=source07, Type=*)),MetricKey(.PrepStmtCacheMissCount,Set(com.bea:Name=source07, Type=*)),Division))\nNamedFeature(Prepared statement cache hit rate,BiCombination(MetricKey(.PrepStmtCacheHitCount,Set(com.bea:Name=source04, Type=*)),MetricKey(.PrepStmtCacheMissCount,Set(com.bea:Name=source04, Type=*)),Division))\nNamedFeature(Rel. unavailable connections,BiCombination(MetricKey(.NumUnavailable,Set(com.bea:Name=source02, Type=JDBCDataSourceRuntime)),MetricKey(.CurrCapacity,Set(com.bea:Name=source02, Type=JDBCDataSourceRuntime)),Division))\nNamedFeature(Prepared statement cache hit rate,BiCombination(MetricKey(.PrepStmtCacheHitCount,Set(com.bea:Name=source09, Type=*)),MetricKey(.PrepStmtCacheMissCount,Set(com.bea:Name=source09, Type=*)),Division))\nNamedFeature(Rel. unavailable connections,BiCombination(MetricKey(.NumUnavailable,Set(com.bea:Name=source03, Type=JDBCDataSourceRuntime)),MetricKey(.CurrCapacity,Set(com.bea:Name=source03, Type=JDBCDataSourceRuntime)),Division))\nNamedFeature(Swap activity,Derivative(MetricKey(.FreeSwapSpaceSize,Set(java.lang:type))))\nNamedFeature(Memory space usage,BiCombination(MetricKey(.Usage.used,Set(java.lang:name=PS*Old*Gen, type=*)),MetricKey(.Usage.max,Set(java.lang:name=PS*Old*Gen, type=*)),Division))\nNamedFeature(Successful wait for connection,Inclination(MetricKey(.WaitingForConnectionSuccessTotal,Set(com.bea:Name, Type))))\nNamedFeature(Objects to be finalized,MetricKey(.ObjectPendingFinalizationCount,Set(java.lang:type)))\nNamedFeature(Memory space activity,Derivative(BiCombination(MetricKey(.Usage.committed,Set(java.lang:name=PS*Perm*Gen, type=*)),MetricKey(.Usage.max,Set(java.lang:name=PS*Perm*Gen, type=*)),Division)))\nNamedFeature(Class unloading activity,Derivative(MetricKey(.UnloadedClassCount,Set(java.lang:type))))\nNamedFeature(Rel. unavailable connections,BiCombination(MetricKey(.NumUnavailable,Set(com.bea:Name=source01, Type=JDBCConnectionPoolRuntime)),MetricKey(.CurrCapacity,Set(com.bea:Name=source01, Type=JDBCConnectionPoolRuntime)),Division))\nNamedFeature(Memory space activity,Derivative(BiCombination(MetricKey(.Usage.committed,Set(java.lang:name=PS*Eden*Space, type=*)),MetricKey(.Usage.max,Set(java.lang:name=PS*Eden*Space, type=*)),Division)))\nNamedFeature(Class loading activity,Derivative(MetricKey(.LoadedClassCount,Set(java.lang:type))))\nNamedFeature(Rel. unavailable connections,BiCombination(MetricKey(.NumUnavailable,Set(com.bea:Name=source07, Type=JDBCDataSourceRuntime)),MetricKey(.CurrCapacity,Set(com.bea:Name=source07, Type=JDBCDataSourceRuntime)),Division))\nNamedFeature(Stuck threads,MetricKey(.StuckThreadCount,Set(com.bea:ApplicationRuntime, Name, Type)))\nNamedFeature(Physical mem activity,Derivative(MetricKey(.FreePhysicalMemorySize,Set(java.lang:type))))\nNamedFeature(Prepared statement cache hit rate,BiCombination(MetricKey(.PrepStmtCacheHitCount,Set(com.bea:Name=source01, Type=*)),MetricKey(.PrepStmtCacheMissCount,Set(com.bea:Name=source01, Type=*)),Division))\nNamedFeature(Rel. unavailable connections,BiCombination(MetricKey(.NumUnavailable,Set(com.bea:Name=source01, Type=JDBCDataSourceRuntime)),MetricKey(.CurrCapacity,Set(com.bea:Name=source01, Type=JDBCDataSourceRuntime)),Division))\nNamedFeature(Rel. unavailable connections,BiCombination(MetricKey(.NumUnavailable,Set(com.bea:Name=source04, Type=JDBCConnectionPoolRuntime)),MetricKey(.CurrCapacity,Set(com.bea:Name=source04, Type=JDBCConnectionPoolRuntime)),Division))\nNamedFeature(Memory space activity,Derivative(BiCombination(MetricKey(.Usage.committed,Set(java.lang:name=Code*Cache, type=*)),MetricKey(.Usage.max,Set(java.lang:name=Code*Cache, type=*)),Division)))\nres39: Int = 90\n"}]},"apps":[],"jobName":"paragraph_1505309655710_-316629721","id":"20170720-165929_1561998452","dateCreated":"2017-09-13T15:34:15+0200","dateStarted":"2017-12-18T11:35:16+0100","dateFinished":"2017-12-18T11:35:20+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:39"},{"text":"%md\n\n# Call of calculation and write to Solr","user":"anonymous","dateUpdated":"2017-12-18T11:34:39+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Call of calculation and write to Solr</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1508831875114_739544424","id":"20171024-095755_1609994658","dateCreated":"2017-10-24T09:57:55+0200","dateStarted":"2017-12-18T11:34:39+0100","dateFinished":"2017-12-18T11:34:39+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:40"},{"text":"%md\n\n[Delete](http://192.168.1.100:8983/solr/featuredata/update?commit=true&stream.body=%3Cdelete%3E%3Cquery%3E*:*%3C%2Fquery%3E%3C%2Fdelete%3E) old docs","user":"anonymous","dateUpdated":"2017-12-18T11:34:39+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><a href=\"http://192.168.1.100:8983/solr/featuredata/update?commit=true&stream.body=%3Cdelete%3E%3Cquery%3E*:*%3C%2Fquery%3E%3C%2Fdelete%3E\">Delete</a> old docs</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1512466580083_-1409516839","id":"20171205-103620_1080877539","dateCreated":"2017-12-05T10:36:20+0100","dateStarted":"2017-12-18T11:34:39+0100","dateFinished":"2017-12-18T11:34:39+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:41"},{"text":"%kryo-spark\nval start = Timestamp.valueOf(\"2014-11-28 00:00:00\")\nval end = Timestamp.valueOf(\"2015-10-04 00:00:00\")\nval writeService = new MetricsServiceImpl.Factory(sqlc).getInstance(\"192.168.1.100:2181\", \"featuredata\")\n\nval processedFeatures = processFeatures(features, readService, start, end)\nval allFeatures = processedFeatures.union(minusOne)\n\nwriteService.putCounters(allFeatures, \"featuredata\", SaveMode.Ignore)","user":"anonymous","dateUpdated":"2017-12-18T11:34:39+0100","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"start: java.sql.Timestamp = 2014-11-28 00:00:00.0\nend: java.sql.Timestamp = 2015-10-04 00:00:00.0\nwriteService: de.qaware.mlwb.api.MetricsService = de.qaware.mlwb.impl.sparksolr.MetricsServiceImpl@2f90e42c\nprocessedFeatures: org.apache.spark.sql.Dataset[de.qaware.mlwb.api.Counter] = [metric: struct<host: string, name: string ... 1 more field>, valuePoints: array<struct<date:timestamp,value:double>>]\nallFeatures: org.apache.spark.sql.Dataset[de.qaware.mlwb.api.Counter] = [metric: struct<host: string, name: string ... 1 more field>, valuePoints: array<struct<date:timestamp,value:double>>]\n"}]},"apps":[],"jobName":"paragraph_1505309655710_-316629721","id":"20170718-141932_1897495706","dateCreated":"2017-09-13T15:34:15+0200","dateStarted":"2017-12-18T11:35:17+0100","dateFinished":"2017-12-18T11:51:23+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:42"},{"text":"","user":"anonymous","dateUpdated":"2017-12-18T11:34:39+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1505309655712_-331250179","id":"20170803-092726_654890728","dateCreated":"2017-09-13T15:34:15+0200","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:43"}],"name":"AnomalyDetectionML/Processing/Feature Calculation","id":"2CS2HX4CK","angularObjects":{"2CZU9K4SP:shared_process":[],"2D18HK2TW:shared_process":[],"2D15EVSM5:shared_process":[],"2D4K6GGH4:shared_process":[],"2D2FQENTY:shared_process":[],"2CYJWH63V:shared_process":[],"2CY5C8M4U:shared_process":[],"2D2NZHKM4:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}